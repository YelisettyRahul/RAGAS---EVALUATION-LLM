{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b1a8439-86b7-4bbf-84aa-5d9d63ea4526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Processed Breast Cancer Research Articles - NCI.docx (168 elements)\n",
      "✅ Processed cancers-15-00321.docx (364 elements)\n",
      "✅ Processed ijo-57-06-1245.docx (292 elements)\n",
      "\n",
      "Total text elements extracted: 824\n",
      "Sample elements:\n",
      "['3/24/25, 9:53 AM\\tBreast Cancer Research Articles - NCI', 'Breast Cancer Research Results and Study Updates', 'See Advances in Breast Cancer Research for an overview of recent']\n"
     ]
    }
   ],
   "source": [
    "from unstructured.partition.auto import partition\n",
    "import os\n",
    "\n",
    "def process_documents(directory_path):\n",
    "    \"\"\"\n",
    "    Process all documents (DOCX, PDF, TXT) in a directory and combine their text content.\n",
    "    \n",
    "    Args:\n",
    "        directory_path (str): Path to the directory containing documents\n",
    "    \n",
    "    Returns:\n",
    "        list: Combined text data from all documents\n",
    "    \"\"\"\n",
    "    supported_extensions = ('.docx', '.pdf', '.txt')\n",
    "    combined_text = []\n",
    "    \n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.lower().endswith(supported_extensions):\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "            try:\n",
    "                # Auto-detect file type and partition\n",
    "                elements = partition(file_path)\n",
    "                # Extract clean text\n",
    "                file_text = [element.text for element in elements if hasattr(element, 'text') and element.text.strip()]\n",
    "                combined_text.extend(file_text)\n",
    "                print(f\"✅ Processed {filename} ({len(file_text)} elements)\")\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Failed to process {filename}: {str(e)}\")\n",
    "    \n",
    "    return combined_text\n",
    "\n",
    "# Example usage:\n",
    "documents_directory = \"all_documents\"\n",
    "all_text_data = process_documents(documents_directory)\n",
    "\n",
    "print(f\"\\nTotal text elements extracted: {len(all_text_data)}\")\n",
    "print(\"Sample elements:\")\n",
    "print(all_text_data[:3])  # Show first 3 elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a215ec2-0bbc-4268-b809-f7c53187d74b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "824"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "529fdad2-18c1-4f1b-88dd-2b1e1f4e82cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import functions / modules\n",
    "from langchain_community.document_loaders import UnstructuredExcelLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain.vectorstores.faiss import FAISS\n",
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fefbc254-5cdb-45d7-806f-39ab3b53032d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.schema import Document\n",
    "\n",
    "def prepare_from_combined_text(combined_text):\n",
    "    \"\"\"\n",
    "    Process already-loaded text data into a FAISS index using PubMedBERT embeddings\n",
    "    \n",
    "    Args:\n",
    "        combined_text (list): List of text strings from previous processing\n",
    "    \n",
    "    Returns:\n",
    "        FAISS: Vector index of all documents\n",
    "    \"\"\"\n",
    "    # Convert raw text to LangChain Documents\n",
    "    documents = [Document(page_content=text) for text in combined_text if text.strip()]\n",
    "    \n",
    "    # Split the text into chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=2000,  # Slightly smaller chunks recommended for PubMedBERT\n",
    "        chunk_overlap=100\n",
    "    )\n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "    \n",
    "    # Initialize PubMedBERT embeddings\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=\"neuml/pubmedbert-base-embeddings\",  # or 'cuda' if available\n",
    "        encode_kwargs={'normalize_embeddings': True}\n",
    "    )\n",
    "    \n",
    "    # Create FAISS index\n",
    "    print(f\"Creating FAISS index with {len(chunks)} chunks using PubMedBERT...\")\n",
    "    return FAISS.from_documents(chunks, embeddings)\n",
    "\n",
    "# Example usage:\n",
    "# faiss_index = prepare_from_combined_text(combined_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "efab07a5-a4de-4b04-8637-2b69f0b22007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating FAISS index with 832 chunks using PubMedBERT...\n"
     ]
    }
   ],
   "source": [
    "df=prepare_from_combined_text(combined_text=all_text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ee32e84d-d907-4c22-a79f-35d1446f18e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "import google.generativeai as genai\n",
    "\n",
    "def ask(df, query, k):\n",
    "    # Retrieve relevant documents with scores\n",
    "    docs_faiss = df.similarity_search_with_score(query, k=k)\n",
    "    \n",
    "    # Prepare context with citations\n",
    "    context_parts = []\n",
    "    source_mapping = {}\n",
    "    \n",
    "    for i, (doc, score) in enumerate(docs_faiss, 1):\n",
    "        source = doc.metadata.get('source', f\"Document {i}\")  # Get filename from metadata\n",
    "        context_parts.append(f\"[[{i}]] {doc.page_content}\")\n",
    "        source_mapping[i] = (source, score)  # Store source and similarity score\n",
    "    \n",
    "    context_text = \"\\n\\n\".join(context_parts)\n",
    "    \n",
    "    # Enhanced prompt with citation instructions\n",
    "    prompt = f\"\"\"\n",
    "    Context information is below. Each section is marked with [[NUMBER]] citations.\n",
    "    ---------------------\n",
    "    {context_text}\n",
    "    ---------------------\n",
    "    Given the context, answer this question: {query}\n",
    "    \n",
    "    Requirements:\n",
    "    1. If the information isn't in the context, say \"I don't have that information\"\n",
    "    2. For any facts used, include [[NUMBER]] citations pointing to which document they came from\n",
    "    3. Include a \"Sources\" section at the end listing all cited documents\n",
    "    4. Keep the answer concise but accurate\n",
    "    \"\"\"\n",
    "    \n",
    "    # Generate response\n",
    "    genai.configure(api_key=\"AIzaSyCXrPcd8h6o0LAcBTtqGDBqCfcLbmNg2-o\")\n",
    "    model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
    "    response = model.generate_content(prompt)\n",
    "    \n",
    "    # Format the output with sources\n",
    "    generated_text = response.candidates[0].content.parts[0].text\n",
    "    \n",
    "    # Add detailed sources section\n",
    "    sources_section = \"\\n\\n## Sources\\n\"\n",
    "    for num, (source, score) in source_mapping.items():\n",
    "        if f\"[[{num}]]\" in generated_text:\n",
    "            sources_section += f\"- [[{num}]] {source} (similarity score: {score:.2f})\\n\"\n",
    "    \n",
    "    full_response = f\"{generated_text}\\n{sources_section}\"\n",
    "    \n",
    "    return display(Markdown(full_response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ce66c1a6-66a8-4427-8f7d-ef199f8f10aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the query\n",
    "import google.generativeai as genai\n",
    "query = \"\"\"\n",
    "What does TNBC stand for?\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6f5311f3-bcac-4968-a23b-a552c6d31db7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "TNBC stands for triple-negative breast cancer. [[4], [5]]\n",
       "\n",
       "Sources:\n",
       "[[2]] Kwa, M.J.; Adams, S. Checkpoint inhibitors in triple-negative breast cancer (TNBC): Where to go from here. Cancer 2018, 124, 2086–2103. [CrossRef]\n",
       "[[3]] Findings from the TAILORx clinical trial show chemotherapy does not benefit most women with early breast cancer. The new data, released at the 2018 ASCO annual meeting, will help inform treatment decisions for many women with early-stage breast cancer.\n",
       "[[4]] TNBC is the subtype of breast cancer with the worst prognosis. To date, although several targeted drugs have been approved for the treatment of TNBC, the urgent need for improved survival has not been met. The practice of immunotherapy in TNBC is just beginning to take off. An advantage of the later start in this field is that experience can be learned from other tumor types, both successful and failed. Although some progress has been made with respect to ICIs for TNBC, many challenges remain. Clinical results show that only a small proportion of patients with TNBC actually benefit from immunotherapy. Thus, identifying the target population and expanding the efficacy is a top priority. Overall, combination treatment is the way forward, but the combination treatment mode, sequence,\n",
       "[[5]] Developing novel treatments in both early and advanced TNBC settings remains a significant unmet need. Recent advances with novel agents have been made for specific subgroups with PD‑L1+ tumors or gBRCAm tumors. However, only a fraction of those patients respond to immune check-point or PARP inhibitors, and even those who do respond often develop resistance and relapse. In diverse tumor microenvironments, a given therapeutic agent shows vari-able responses, thus compromising the survival endpoints especially in an unselected TNBC population. Therefore, developing novel predictive biomarkers are crucial for selecting patients that will benefit the most from a given therapy. Single cell technologies will provide additional insight on tumor‑stroma interactions and facilitate compelling rationale for new treatments based on novel biomarkers. A non‑invasive testing of plasma circulating tumor DNA (ctDNA) and CTCs can potentially provide real ‑time disease monitoring and even early therapy modification. However, their prognostic value needs further evaluation. With recent advances in multiomic analyses of cancers, there appears to be genomic and molecular similarities between TNBC and high ‑grade serous ovarian carcinoma (HGSOC), suggesting that similar biological mechanisms drive some aspects of both cancer types. Therefore, treatment strategies for HGSOC can be explored in TNBC as well. The recent increase in the number of clinical trials investigating various new agents and combination strategies reflects further efforts to understand molecular and immunological aspects of TNBC. This may lead to more meaningful clinical benefits, including event‑free and overall survival.\n",
       "[[1]] The test, which helps guide treatment decisions, was not as good at predicting the risk of death from breast cancer for Black patients as for White patients, a new study has found. The ndings highlight the need for greater racial diversity in research studies.\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "## Sources\n",
       "- [[1]] Document 1 (similarity score: 0.95)\n",
       "- [[2]] Document 2 (similarity score: 0.97)\n",
       "- [[3]] Document 3 (similarity score: 0.98)\n",
       "- [[4]] Document 4 (similarity score: 1.05)\n",
       "- [[5]] Document 5 (similarity score: 1.06)\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ask(df,query,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e017d36b-26d4-4287-a012-ddc83c307e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "import google.generativeai as genai\n",
    "\n",
    "def ask(df, query, k=3, score_threshold=0.7):\n",
    "    \"\"\"\n",
    "    Enhanced RAG function with:\n",
    "    - PDF page number citations\n",
    "    - Document titles in metadata\n",
    "    - Score threshold filtering\n",
    "    - Beautiful source attribution\n",
    "    \n",
    "    Args:\n",
    "        df: FAISS vector store\n",
    "        query: User question\n",
    "        k: Number of docs to retrieve\n",
    "        score_threshold: Minimum similarity score (0-1)\n",
    "    \"\"\"\n",
    "    # Retrieve documents with scores\n",
    "    docs_faiss = df.similarity_search_with_score(query, k=k)\n",
    "    \n",
    "    # Filter by score threshold and prepare context\n",
    "    context_parts = []\n",
    "    source_mapping = {}\n",
    "    valid_docs = []\n",
    "    \n",
    "    for i, (doc, score) in enumerate(docs_faiss, 1):\n",
    "        if score < score_threshold:\n",
    "            continue\n",
    "            \n",
    "        valid_docs.append((doc, score))\n",
    "        \n",
    "        # Extract metadata with fallbacks\n",
    "        metadata = doc.metadata\n",
    "        title = metadata.get('title', \"Untitled Document\")\n",
    "        source = metadata.get('source', \"Unknown Source\")\n",
    "        page = metadata.get('page', \"N/A\")\n",
    "        \n",
    "        # Create citation label\n",
    "        citation_id = f\"[Doc{i}]\"\n",
    "        context_parts.append(f\"{citation_id}\\n{doc.page_content}\")\n",
    "        \n",
    "        # Store source info\n",
    "        source_mapping[citation_id] = {\n",
    "            'title': title,\n",
    "            'source': source,\n",
    "            'page': page,\n",
    "            'score': f\"{score:.2f}\"\n",
    "        }\n",
    "    \n",
    "    if not valid_docs:\n",
    "        return display(Markdown(\"🔍 No relevant documents found above similarity threshold.\"))\n",
    "    \n",
    "    context_text = \"\\n\\n---\\n\\n\".join(context_parts)\n",
    "    \n",
    "    # Enhanced prompt\n",
    "    prompt = f\"\"\"You're a research assistant. Answer using ONLY these verified sources:\n",
    "\n",
    "{context_text}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Guidelines:\n",
    "1. ALWAYS cite sources like this: [Doc1]\n",
    "2. For page references: [Doc1, p.3]\n",
    "3. If unsure, say \"I couldn't find definitive evidence\"\n",
    "4. Include ALL relevant citations\n",
    "5. Structure your answer clearly\n",
    "\n",
    "Final Answer:\"\"\"\n",
    "    \n",
    "    # Generate response\n",
    "    genai.configure(api_key=\"AIzaSyCXrPcd8h6o0LAcBTtqGDBqCfcLbmNg2-o\")\n",
    "    model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
    "    response = model.generate_content(prompt)\n",
    "    generated_text = response.text\n",
    "    \n",
    "    # Build beautiful sources section\n",
    "    sources_section = \"\\n\\n### References\\n\"\n",
    "    for citation_id, info in source_mapping.items():\n",
    "        if citation_id in generated_text:\n",
    "            sources_section += (\n",
    "                f\"- {citation_id}: {info['title']}\\n\"\n",
    "                f\"  - Source: {info['source']}\\n\"\n",
    "                f\"  - Page: {info['page']}\\n\"\n",
    "                f\"  - Similarity: {info['score']}\\n\\n\"\n",
    "            )\n",
    "    \n",
    "    # Combine everything\n",
    "    full_response = f\"{generated_text}\\n{sources_section}\"\n",
    "    return display(Markdown(full_response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f153a28a-3cea-442e-bbdf-54a770cc02dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "I couldn't find definitive evidence for what TNBC stands for in the provided source documents.  While Doc2 mentions \"triple-negative breast cancer (TNBC)\", it does not provide the full meaning of the acronym.\n",
       "\n",
       "\n",
       "\n",
       "### References\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ask(df,query,k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c197ea0b-e665-4042-8b29-599501e21a53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
